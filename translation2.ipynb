{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8739747",
   "metadata": {},
   "source": [
    "# Library Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "055ed441",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import numpy as np\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from collections import Counter\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Model\n",
    "from keras.layers import LSTM, Input, TimeDistributed, Dense, Activation, RepeatVector, Embedding\n",
    "from keras.optimizers import Adam\n",
    "from keras.losses import sparse_categorical_crossentropy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67bc4651",
   "metadata": {},
   "source": [
    "# Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ee8f1ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to translation file\n",
    "path_to_data = 'C:\\\\Users\\\\tessw\\\\OneDrive\\\\Documents\\\\University\\\\Honours\\\\hau.txt'\n",
    "\n",
    "# Read file\n",
    "translation_file = open(path_to_data, \"r\", encoding='utf-8') \n",
    "raw_data = translation_file.read()\n",
    "translation_file.close()\n",
    "\n",
    "# Parse data\n",
    "raw_data = raw_data.split('\\n')\n",
    "pairs = [sentence.split('\\t') for sentence in  raw_data]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6461904e",
   "metadata": {},
   "source": [
    "# Text Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4e0bdeb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_sentence(sentence):\n",
    "    # Lower case the sentence\n",
    "    lower_case_sent = sentence.lower()\n",
    "    # Strip punctuation\n",
    "    string_punctuation = string.punctuation + \"!\" + '?'\n",
    "    clean_sentence = lower_case_sent.translate(str.maketrans('', '', string_punctuation))\n",
    "   \n",
    "    return clean_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d354992d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(sentences):\n",
    "    # Create tokenizer\n",
    "    text_tokenizer = Tokenizer()\n",
    "    # Fit texts\n",
    "    text_tokenizer.fit_on_texts(sentences)\n",
    "    return text_tokenizer.texts_to_sequences(sentences), text_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d2ddb125",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum length hausa sentence: 89\n",
      "Maximum length english sentence: 72\n",
      "Hausa vocabulary is of 1014 unique words\n",
      "English vocabulary is of 977 unique words\n"
     ]
    }
   ],
   "source": [
    "# Clean sentences\n",
    "english_sentences = [clean_sentence(pair[0]) for pair in pairs]\n",
    "hausa_sentences = [clean_sentence(pair[1]) for pair in pairs]\n",
    "\n",
    "# Tokenize words\n",
    "hau_text_tokenized, hau_text_tokenizer = tokenize(hausa_sentences)\n",
    "eng_text_tokenized, eng_text_tokenizer = tokenize(english_sentences)\n",
    "\n",
    "print('Maximum length hausa sentence: {}'.format(len(max(hau_text_tokenized,key=len))))\n",
    "print('Maximum length english sentence: {}'.format(len(max(eng_text_tokenized,key=len))))\n",
    "\n",
    "# Check language length\n",
    "hausa_vocab = len(hau_text_tokenizer.word_index) + 1\n",
    "english_vocab = len(eng_text_tokenizer.word_index) + 1\n",
    "print(\"Hausa vocabulary is of {} unique words\".format(hausa_vocab))\n",
    "print(\"English vocabulary is of {} unique words\".format(english_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "abf221c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_hausa_len = int(len(max(hau_text_tokenized,key=len)))\n",
    "max_english_len = int(len(max(eng_text_tokenized,key=len)))\n",
    "\n",
    "hau_pad_sentence = pad_sequences(hau_text_tokenized, max_hausa_len, padding = \"post\")\n",
    "eng_pad_sentence = pad_sequences(eng_text_tokenized, max_english_len, padding = \"post\")\n",
    "\n",
    "# Reshape data\n",
    "hau_pad_sentence = hau_pad_sentence.reshape(*hau_pad_sentence.shape, 1)\n",
    "eng_pad_sentence = eng_pad_sentence.reshape(*eng_pad_sentence.shape, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb4edb1e",
   "metadata": {},
   "source": [
    "# Model Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "19c75766",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sequence = Input(shape=(max_hausa_len,))\n",
    "embedding = Embedding(input_dim=hausa_vocab, output_dim=128,)(input_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ec790a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sequence = Input(shape=(max_hausa_len,))\n",
    "embedding = Embedding(input_dim=hausa_vocab, output_dim=128,)(input_sequence)\n",
    "encoder = LSTM(64, return_sequences=False)(embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "641bf142",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sequence = Input(shape=(max_hausa_len,))\n",
    "embedding = Embedding(input_dim=hausa_vocab, output_dim=128,)(input_sequence)\n",
    "encoder = LSTM(64, return_sequences=False)(embedding)\n",
    "r_vec = RepeatVector(max_english_len)(encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "26312eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sequence = Input(shape=(max_hausa_len,))\n",
    "embedding = Embedding(input_dim=hausa_vocab, output_dim=128,)(input_sequence)\n",
    "encoder = LSTM(64, return_sequences=False)(embedding)\n",
    "r_vec = RepeatVector(max_english_len)(encoder)\n",
    "decoder = LSTM(64, return_sequences=True, dropout=0.2)(r_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b962fe34",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sequence = Input(shape=(max_hausa_len,))\n",
    "embedding = Embedding(input_dim=hausa_vocab, output_dim=128,)(input_sequence)\n",
    "encoder = LSTM(64, return_sequences=False)(embedding)\n",
    "r_vec = RepeatVector(max_english_len)(encoder)\n",
    "decoder = LSTM(64, return_sequences=True, dropout=0.2)(r_vec)\n",
    "logits = TimeDistributed(Dense(english_vocab))(decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d90762e",
   "metadata": {},
   "source": [
    "# Model Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6f056c9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_5 (InputLayer)        [(None, 89)]              0         \n",
      "                                                                 \n",
      " embedding_4 (Embedding)     (None, 89, 128)           129792    \n",
      "                                                                 \n",
      " lstm_4 (LSTM)               (None, 64)                49408     \n",
      "                                                                 \n",
      " repeat_vector_2 (RepeatVect  (None, 72, 64)           0         \n",
      " or)                                                             \n",
      "                                                                 \n",
      " lstm_5 (LSTM)               (None, 72, 64)            33024     \n",
      "                                                                 \n",
      " time_distributed (TimeDistr  (None, 72, 977)          63505     \n",
      " ibuted)                                                         \n",
      "                                                                 \n",
      " activation (Activation)     (None, 72, 977)           0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 275,729\n",
      "Trainable params: 275,729\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "enc_dec_model = Model(input_sequence, Activation('softmax')(logits))\n",
    "enc_dec_model.compile(loss=sparse_categorical_crossentropy,\n",
    "              optimizer=Adam(1e-3),\n",
    "              metrics=['accuracy'])\n",
    "enc_dec_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb50aebb",
   "metadata": {},
   "source": [
    "# Run Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7ed95531",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_results = enc_dec_model.fit(hau_pad_sentence, eng_pad_sentence, batch_size=70, epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a16c7105",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logits_to_sentence(logits, tokenizer):\n",
    "\n",
    "    index_to_words = {idx: word for word, idx in tokenizer.word_index.items()}\n",
    "    index_to_words[0] = '' \n",
    "\n",
    "    return ' '.join([index_to_words[prediction] for prediction in np.argmax(logits, 1)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4be4cdb",
   "metadata": {},
   "source": [
    "# Output Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c888fc0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 1\n",
    "\n",
    "def eng_to_hau():\n",
    "    print()\n",
    "    print(\"The hausa sentence is: {}\".format(hausa_sentences[index]))\n",
    "    print()\n",
    "    print('The predicted sentence is :')\n",
    "    print(logits_to_sentence(enc_dec_model.predict(eng_pad_sentence[index:index+1])[0], hau_text_tokenizer))\n",
    "    print()\n",
    "    print(\"The english sentence is: {}\".format(english_sentences[index]))\n",
    "    \n",
    "def hau_to_eng():\n",
    "    print(\"The hausa sentence is: {}\".format(hausa_sentences[index]))\n",
    "    print()\n",
    "    print('The predicted sentence is :')\n",
    "    print(logits_to_sentence(enc_dec_model.predict(hau_pad_sentence[index:index+1])[0], eng_text_tokenizer))\n",
    "    print()\n",
    "    print(\"The english sentence is: {}\".format(english_sentences[index]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f03b24cc",
   "metadata": {},
   "source": [
    "# Translation Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "bf02b1ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Do you want to translate Eng-Hau (enter E) or Hau-Eng? (enter H)\n",
      "h\n",
      "The hausa sentence is: new delhi ap  firayim ministan indiya narendra modi zai halarci bikin aza harsashin ginin wurin bautar hindu a watan mai zuwa a wani wurin da ake takaddama da ke arewacin indiya a inda a da wannan wurin wania masallaci ne aka rugurguza shi karni na 16 wanda wasu masu tsattsauran raayi na hindu su ka yi a 1992 kamar yadda aka ji daga masu amintattu daaka dorawa alhakkin kula da ginin\n",
      "\n",
      "The predicted sentence is :\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "the delhi ap indian prime minister narendra modi greece attend the groundbreaking groundbreaking next month the and on on on a disputed the the northern india where a 16th century century was torn down to hindu hardliners 1992 1992 according the the the overseeing the construction construction                         \n",
      "\n",
      "The english sentence is: new delhi ap  indian prime minister narendra modi will attend a groundbreaking ceremony next month for a hindu temple on a disputed site in northern india where a 16th century mosque was torn down by hindu hardliners in 1992 according to the trust overseeing the temple construction\n"
     ]
    }
   ],
   "source": [
    "print(\"Do you want to translate Eng-Hau (enter E) or Hau-Eng? (enter H)\")\n",
    "choice = input()\n",
    "\n",
    "if choice == 'E' or choice == 'e':\n",
    "    eng_to_hau()\n",
    "elif choice == 'H' or choice == 'h':\n",
    "    hau_to_eng()\n",
    "else:\n",
    "    print(\"Invalid choice, please enter E or H.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
